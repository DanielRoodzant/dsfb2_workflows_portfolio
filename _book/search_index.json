[["index.html", "R portfolio Daniël Roodzant Introduction", " R portfolio Daniël Roodzant Daniël Roodzant 2021-06-23 Introduction Welcome to my bookdown portfolio. Here I will show you some of the coding skills I have gathered. This portfolio has been created with the help of the drporthelp package which I have built to reduce the repeating of code. Lets start with my CV which I have created using the pagedown package in R: For Dutch: Dutch CV For English: English CV "],["c-elegans-plate-experiment.html", "C. elegans plate experiment", " C. elegans plate experiment The data for this exercise was kindly supplied by J. Louter (INT/ILC) and was derived from an experiment in which adult C.elegans nematodes were exposed to varying concentrations of different compounds. The variables RawData (the outcome - number of offspring counted as an integer value, after incubation time), compName (the generic name of the compound/chemical), the compConcentration (the concentration of the compound), and the expType are the most important variables in this dataset. A typical analysis with this data would be to run a dose-response analysis using a log-logistic model with estimates for the maximal, the minimal, the IC50 concentration and the slope at IC50. We will not go into the details but a good package to run such computations and create graphs in R is the {drc} package. Lets have a look at the excel file containing the data. We can see that someone put a lot of care into the make-up of the sheet and it is still difficult to read. Now lets open the Excel file in R and render the first five rows. # Read the excel file into a vector and display it in a small table. scatter_FLOW.062 &lt;- read_excel(&#39;data/data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&#39;) knitr::kable( scatter_FLOW.062 %&gt;% head(5)) plateRow plateColumn vialNr dropCode expType expReplicate expName expDate expResearcher expTime expUnit expVolumeCounted RawData compCASRN compName compConcentration compUnit compDelivery compVehicle elegansStrain elegansInput bacterialStrain bacterialTreatment bacterialOD600 bacterialConcX bacterialVolume bacterialVolUnit incubationVial incubationVolume incubationUnit incubationMethod incubationRPM bubble incubateTemperature NA NA 1 a experiment 3 CE.LIQ.FLOW.062 2020-11-30 Sergio Reijnders - Ellis Herder 68 hour 50 44 24157-81-1 2,6-diisopropylnaphthalene 4.99 nM Liquid controlVehicleA N2 25 OP50 heated 0.743 8 300 ul 1,5 glass vial 1000 ul rockroll 35 NA 20 NA NA 1 b experiment 3 CE.LIQ.FLOW.062 2020-11-30 Sergio Reijnders - Ellis Herder 68 hour 50 37 24157-81-1 2,6-diisopropylnaphthalene 4.99 nM Liquid controlVehicleA N2 25 OP50 heated 0.743 8 300 ul 1,5 glass vial 1000 ul rockroll 35 NA 20 NA NA 1 c experiment 3 CE.LIQ.FLOW.062 2020-11-30 Sergio Reijnders - Ellis Herder 68 hour 50 45 24157-81-1 2,6-diisopropylnaphthalene 4.99 nM Liquid controlVehicleA N2 25 OP50 heated 0.743 8 300 ul 1,5 glass vial 1000 ul rockroll 35 NA 20 NA NA 1 d experiment 3 CE.LIQ.FLOW.062 2020-11-30 Sergio Reijnders - Ellis Herder 68 hour 50 47 24157-81-1 2,6-diisopropylnaphthalene 4.99 nM Liquid controlVehicleA N2 25 OP50 heated 0.743 8 300 ul 1,5 glass vial 1000 ul rockroll 35 NA 20 NA NA 1 e experiment 3 CE.LIQ.FLOW.062 2020-11-30 Sergio Reijnders - Ellis Herder 68 hour 50 41 24157-81-1 2,6-diisopropylnaphthalene 4.99 nM Liquid controlVehicleA N2 25 OP50 heated 0.743 8 300 ul 1,5 glass vial 1000 ul rockroll 35 NA 20 And have a look at the data types. # Display the data type of select columns scatter_FLOW.062 %&gt;% select(RawData, compName, compConcentration) %&gt;% head(1) ## # A tibble: 1 x 3 ## RawData compName compConcentration ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 44 2,6-diisopropylnaphthalene 4.99 When we have a look at some of the data types of the collumns RawData, compName and compConcentration (see above) we would expect them to be dbl, chr and dbl. compConcentration however, has the datatype chr. This means it was imported incorrectly. Next we will create a scatterplot graph with the data for the different compounds and the varying concentrations. To do this I first fixed the compConcentration column by making it numeric, put the compConcentration on the x-axis, the RawData counts on the y-axis and assigned a color to each level in compName. I also assigned a different symbol to each level by the expType variable. # Transform the datatype of compConcentration to dbl scatter_FLOW.062 &lt;- scatter_FLOW.062 %&gt;% transform(compConcentration = as.double(compConcentration)) # Plotting data using ggplot scatter_FLOW.062 %&gt;% ggplot(aes(x = compConcentration, y = RawData)) + geom_point(aes(colour = compName, shape = expType)) + labs(title = &quot;Compound RawData per compound concentration&quot;, caption = &quot;Data supplied by J. Louter (INT/ILC)&quot;) If I would not have changed the data type of the compConcentration column this would have happened: # Show example of the wrong graph from unedited .xlsx format by retrieving data wrong_scatter_FLOW.062 &lt;- read_excel(&#39;data/data_raw/CE.LIQ.FLOW.062_Tidydata.xlsx&#39;) # Plotting data using ggplot wrong_scatter_FLOW.062 %&gt;% ggplot(aes(x = compConcentration, y = RawData)) + geom_point(aes(colour = compName, shape = expType)) + labs(title = &quot;Compound RawData per compound concentration&quot;, caption = &quot;Data supplied by J. Louter (INT/ILC)&quot;) Every concentration is seen as a separate point because the column has the chr type. Now, with the correct data types we will tweak the graph using a log10 transformation on the x-axis to get a clear graph. I also added a bit of jitter to prevent the points in the graph from overlapping. # Plot data using log10 function in ggplot scatter_FLOW.062 %&gt;% ggplot(aes(x = compConcentration, y = RawData)) + geom_point() + geom_jitter(aes(colour = compName, shape = expType), width = 0.5) + scale_x_log10() + labs(title = &quot;Compound RawData per compound concentration&quot;, caption = &quot;Data supplied by J. Louter (INT/ILC)&quot;) The positive control for this experiments is ethanol. The negative control for this experiment is S-medium. To analyze this experiment and learn whether there is indeed an effect of different concentrations on offspring count and whether the different compounds have a different curve (IC50) I would take these steps: - Group the data for every compound. - Check if the data is normally distributed. - Use the apropriate statistical tests on the data to see if there is a statistically significant effect of different concentrations on the offspring count. - Calculate the IC50 and create a plot of the curve. For the next vizualization of the data I have normalised the data Normalize the data for the controlNegative in such a way that the mean value for controlNegative is exactly equal to 1 and that all other values are expressed as a fraction thereof. Rerun your graphs with the normalized data. # Calculate the mean of the RawData mean_data_FLOW.062 &lt;- scatter_FLOW.062 %&gt;% select(expType, RawData, compName, compConcentration) %&gt;% group_by(expType) %&gt;% filter(expType == &#39;controlNegative&#39;) %&gt;% summarise(mean_RawData = mean(RawData, na.rm = TRUE)) # Normalise the RawData using the calculated mean normalized_FLOW.062 &lt;- scatter_FLOW.062 %&gt;% select(expType, compName, compConcentration, RawData) %&gt;% mutate(RawData_normalized = RawData / mean_data_FLOW.062$mean_RawData) # Calculate the mean of the normalised data mean_normalized_FLOW.062 &lt;- normalized_FLOW.062 %&gt;% group_by(expType, compName, compConcentration) %&gt;% summarise(mean_RawData_normalized = mean(RawData_normalized, na.rm = TRUE)) knitr::kable(mean_normalized_FLOW.062 %&gt;% head(5)) expType compName compConcentration mean_RawData_normalized controlNegative S-medium 0.00e+00 1.0000000 controlPositive Ethanol 1.50e+00 0.5750873 controlVehicleA Ethanol 5.00e-01 1.0690726 experiment 2,6-diisopropylnaphthalene 4.99e-05 1.0391929 experiment 2,6-diisopropylnaphthalene 4.99e-04 0.9670159 # Plot the normalised mean using ggplot mean_normalized_FLOW.062 %&gt;% ggplot(aes(x = compConcentration, y = mean_RawData_normalized)) + geom_point() + geom_jitter(aes(colour = compName, shape = expType), width = 0.5) + labs(title = &quot;Normalized mean compound RawData per compound concentration&quot;, caption = &quot;Data supplied by J. Louter (INT/ILC)&quot;) I took this step to get the result in relation to the 0 value which makes it easier to compare the samples to the normal value. "],["open-peer-review.html", "Open Peer Review", " Open Peer Review This exercise is about identifying reproducibility issues in a scientific publication. We use the criteria for reproduciblity that are publicly available. (#tab:read criteria table)Reproducibility criteria Transparency Criteria Definition Response Type Study Purpose A concise statement in the introduction of the article, often in the last paragraph, that establishes the reason the research was conducted. Also called the study objective. Binary Data Availability Statement A statement, in an individual section offset from the main body of text, that explains how or if one can access a studys data. The title of the section may vary, but it must explicitly mention data; it is therefore distinct from a supplementary materials section. Binary Data Location Where the articles data can be accessed, either raw or processed. Found Value Study Location Author has stated in the methods section where the study took place or the datas country/region of origin. Binary; Found Value Author Review The professionalism of the contact information that the author has provided in the manuscript. Found Value Ethics Statement A statement within the manuscript indicating any ethical concerns, including the presence of sensitive data. Binary Funding Statement A statement within the manuscript indicating whether or not the authors received funding for their research. Binary Code Availability Authors have shared access to the most updated code that they used in their study, including code used for analysis. Binary Table clarification The Transparency Criteria are criteria needed to score an article of your choice. Each Transparency criterion comes with a Definition that explains the criterion in more details. These descriptions are particularly helpful to understand what the criterion entails and what to look for in the article. The Response Type is the actual score. resource I found an open access scientific article using PubMed and scored it using the table above. Article: Yamayoshi S, Sakai-Tagawa Y, Koga M, et al. Comparison of Rapid Antigen Tests for COVID-19. Viruses. 2020;12(12):1420. Published 2020 Dec 10. doi:10.3390/v12121420 Study purpose - Present Data availability statement - Not present Data location - Present Study location - Present Author review - Not present Ethics statement - Present Funding statement - Present Code availability - Not present In the study rapid antigen tests (RAT) for covid-19 are tested on corona positive patients to compare their effectivity to the PCR method. To do this patient material such as saliva and nose/throat swabs were used with four types of RATs and the standard PCR test. The RATs were also tested using 2 isolated virus strands. The RATs were less sensitive than the PCR test. Some of the RATs are not able to detect low quantities of virusparticles. There is no big difference between the isolated virus and the clinical samples. Data is available onine here Next I used the OSF website to select a project that adresses an aspect of the SARS-Cov-2 virus and contained a dataset and R-code in the project enviorment. I found the project Bats and COVID-19 The code in this project intends to compare the search words coronavirus and bats through Google and wikipedia in multiple countries and score how often they were used together. The readability of the code is a 4/5. Click here to see the full code from the article. &lt;br Now lets reproduce some of the code: ### Display code chunk from bats_and_covid.R file.### # Fig.1 (2016-2020 US Tv news about bats) {.unnumbered} { tv.dat &lt;- read.csv(file=&quot;data/data_raw/GDELTBatsUS1620.csv&quot;) tv.dat$date &lt;- ymd(tv.dat$date) tv.dat[61, &quot;date&quot;] &lt;- &quot;2021-01-01&quot; tv.dat[61, &quot;value&quot;] &lt;- NA tv.dat[61, &quot;X&quot;] &lt;- 61 ggplot() + geom_line(data=tv.dat, aes(x=X, y=value), size=0.8, color=&quot;steelblue&quot;, linetype=&quot;solid&quot;) + theme_bw() + scale_x_continuous(breaks=c(1, 13, 25, 37, 49, 61), labels=c(&quot;2016&quot;, &quot;2017&quot;, &quot;2018&quot;, &quot;2019&quot;, &quot;2020&quot;, &quot;2021&quot;)) + theme(axis.text.x = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.text.y = element_text(colour=&quot;black&quot;,size=14,angle=0,hjust=1,vjust=0,face=&quot;plain&quot;), axis.title.x = element_text(colour=&quot;black&quot;,size=12,angle=0,hjust=.5,vjust=.5,face=&quot;plain&quot;), axis.title.y = element_text(colour=&quot;black&quot;,size=12,angle=90,hjust=.5,vjust=.5,face=&quot;plain&quot;), plot.title = element_text(size=22, face=&quot;bold&quot;)) + theme(strip.background =element_rect(fill=&quot;wheat&quot;)) + labs(x=&quot;&quot;, y=&quot;&quot;) + theme(strip.text = element_text(colour = &quot;black&quot;, size=14, face=&quot;bold&quot;)) } Errors: Adjusted data location and loaded library lubridate. It took very low effort (4/5) to reproduce the code. Lack of pseudo coding makes the data somewhat more difficult to interpret. "],["guerrilla-analytics-framework.html", "Guerrilla analytics framework", " Guerrilla analytics framework An important part of having a project repository is to keep it easily accesible and clean. This will allow anyone who uses the repository to be able to easily find their way around the files and data. The data structure in the repository used for this portfolio is based on the Guerilla Analytics Principle. An example of this van be found below: ## . ## +-- 001_c_elegans_plate_exp.Rmd ## +-- 002_open_peer_review.Rmd ## +-- 003_guerilla_analytics.Rmd ## +-- 004_projecticum_pmc.Rmd ## +-- 005_relational_databases.Rmd ## +-- 006_parameterized_data.Rmd ## +-- 007_looking_ahead.Rmd ## +-- data ## | +-- CV_EN ## | | +-- _CV_Daniël_Roodzant_EN.html ## | | +-- _CV_Daniël_Roodzant_EN.pdf ## | | +-- _CV_Daniël_Roodzant_EN.Rmd ## | | \\-- _CV_Daniël_Roodzant_EN_files ## | | +-- font-awesome-5.1.0 ## | | | +-- css ## | | | | +-- all.css ## | | | | \\-- v4-shims.css ## | | | \\-- webfonts ## | | | +-- fa-brands-400.eot ## | | | +-- fa-brands-400.svg ## | | | +-- fa-brands-400.ttf ## | | | +-- fa-brands-400.woff ## | | | +-- fa-brands-400.woff2 ## | | | +-- fa-regular-400.eot ## | | | +-- fa-regular-400.svg ## | | | +-- fa-regular-400.ttf ## | | | +-- fa-regular-400.woff ## | | | +-- fa-regular-400.woff2 ## | | | +-- fa-solid-900.eot ## | | | +-- fa-solid-900.svg ## | | | +-- fa-solid-900.ttf ## | | | +-- fa-solid-900.woff ## | | | \\-- fa-solid-900.woff2 ## | | +-- header-attrs-2.8 ## | | | \\-- header-attrs.js ## | | \\-- paged-0.14 ## | | +-- css ## | | | \\-- resume.css ## | | \\-- js ## | | +-- config.js ## | | +-- hooks.js ## | | \\-- paged.js ## | +-- CV_NL ## | | +-- _CV_Daniël_Roodzant_NL.html ## | | +-- _CV_Daniël_Roodzant_NL.pdf ## | | +-- _CV_Daniël_Roodzant_NL.Rmd ## | | \\-- _CV_Daniël_Roodzant_NL_files ## | | +-- font-awesome-5.1.0 ## | | | +-- css ## | | | | +-- all.css ## | | | | \\-- v4-shims.css ## | | | \\-- webfonts ## | | | +-- fa-brands-400.eot ## | | | +-- fa-brands-400.svg ## | | | +-- fa-brands-400.ttf ## | | | +-- fa-brands-400.woff ## | | | +-- fa-brands-400.woff2 ## | | | +-- fa-regular-400.eot ## | | | +-- fa-regular-400.svg ## | | | +-- fa-regular-400.ttf ## | | | +-- fa-regular-400.woff ## | | | +-- fa-regular-400.woff2 ## | | | +-- fa-solid-900.eot ## | | | +-- fa-solid-900.svg ## | | | +-- fa-solid-900.ttf ## | | | +-- fa-solid-900.woff ## | | | \\-- fa-solid-900.woff2 ## | | +-- header-attrs-2.8 ## | | | \\-- header-attrs.js ## | | \\-- paged-0.14 ## | | +-- css ## | | | \\-- resume.css ## | | \\-- js ## | | +-- config.js ## | | +-- hooks.js ## | | \\-- paged.js ## | \\-- data_raw ## | +-- bats_and_covid.R ## | +-- bibliography_pmc_proj.bib ## | +-- CE.LIQ.FLOW.062_Tidydata.xlsx ## | +-- coursera_ex1.png ## | +-- covid_params_data ## | +-- DBeaver_example.png ## | +-- dengue_data.csv ## | +-- dengue_data_tidy.csv ## | +-- dengue_data_tidy.rds ## | +-- flu_data.csv ## | +-- flu_data_tidy.csv ## | +-- flu_data_tidy.rds ## | +-- gapminder.csv ## | +-- gapminder.rds ## | +-- GDELTBatsUS1620.csv ## | +-- reproducibility_criteria.xlsx ## | +-- sql_shell.png ## | \\-- Untidy_example.png ## +-- index.Rmd ## +-- README.md ## +-- WorkflowsPortfolio.Rproj ## +-- _book ## +-- _bookdown_files ## +-- _main.Rmd ## \\-- _main_files ## \\-- figure-html ## +-- 1.1.d-1.png ## +-- 1.1.e-1.png ## +-- 1.1.f-1.png ## +-- 1.1.j-1.png ## +-- bats_and_covid.R code-1.png ## +-- creating graphs-1.png ## +-- creating graphs-2.png ## +-- creating informative graphs-1.png ## +-- creating informative graphs-2.png ## +-- data visualisation 1-1.png ## +-- data visualisation 2-1.png ## +-- data visualisation 3-1.png ## \\-- data visualisation-1.png "],["projecticum-princes-maxima-center.html", "Projecticum Princes Maxima Center References", " Projecticum Princes Maxima Center During the minor Data Science for Biology at Utrecht University of Applied Science we got an externally provided projecticum which in our case came from the Princess Maxima Center (PMC) in Utrecht. The projecticum assignment was to create a Rshiny app containing an interactive dashboard that displays the metadata of tests run by researchers from the PMC. To create the app our group first had to get familiar with RShiny. Luckily the internet is full of pages on how to use shiny and its many applications. This: (Mastering-shiny.org n.d.) online course was the biggest help in learning how to use shiny as it walks you through the basics as well as the more advanced parts of shiny. Another great help was this video: (Shiny Rstudio tutorial, n.d.) containing a tutorial on how to work with Rshiny, and this cheat sheet: (Shiny cheatsheet, n.d.) containing some usefull commands to use for Rshiny. After learning how to create Rshiny applications it was time to start working on the product. The workflow was organised to be agile and was run through a GitHub projectboard containing all the tasks needed to complete the product. The tasks were completed in sprints with occasional scrum meetings between the group members. When used the app will ask the user to upload files out of which the application builds an interactive dataframe. The dataframe contains all the data points required for the metadata file and can be modified easily using dropdown boxes and power functions to modify entire rows. Another feature was an interactive graph of a 384-well plate in which users can handselect wells to exclude from the dataset. Once the user is done filling in the dataframe he or she can press a download button to download a TSV formatted .txt file of the data to store locally. During the process of creating the dashboard there was frequent contact with the representative from the PMC so he could express his thoughts on the app and the direction it was headed in. This ensured the product would be user friendly for those who would actually use it. Other great links are: Shiny gallery with great apps other people made: (Shiny Rstudio tutorial Gallery, n.d.) Great app from the gallery: (Shiny Rstudio Pokémon app, n.d.) References "],["relational-databases.html", "Relational databases Intro Cleaning up Using DBeaver SQL Joining the data Visualising the data", " Relational databases Intro On this page we will have a look at flu and dengue occurrences between 2002 and 2015 as measured by Google webservices Google Flu Trends and Google Dengue Trends. These webservices track searches of the words flu and dengue in different countries and match them to occurrences of the diseases in these countries The data will be compared to the gapminder data set that is part of the R package dslabs. Data Source: (Google sites are no longer operational) - Google Flu Trends (http://www.google.org/flutrends) - Google Dengue Trends (http://www.google.org/denguetrends) - dslabs package Datasets used: - Flu data - Dengue data - gapminder Cleaning up Before any comparisons can be made the data must be made tidy and the column types must be matching. The gapminder dataset is also very large and includes a large number of years not present in the flu and dengue datasets. These years also need to be removed from the dataset. The next section will show how this was achieved: # Load the datasets into R datframes flu_data &lt;- read.csv(&#39;data/data_raw/flu_data.csv&#39;, skip = 11) %&gt;% as_tibble() dengue_data &lt;- read.csv(&quot;data/data_raw/dengue_data.csv&quot;, skip = 11) %&gt;% as_tibble() gapminder &lt;- dslabs::gapminder %&gt;% as_tibble() # Tidy the flu_data and dengue_data datasets flu_data &lt;- flu_data %&gt;% tidyr::pivot_longer(cols = Argentina:Uruguay, names_to = &quot;country&quot;, values_to = &quot;searches&quot; ) dengue_data &lt;- dengue_data %&gt;% tidyr::pivot_longer(cols = Argentina:Venezuela, names_to = &quot;country&quot;, values_to = &quot;searches&quot; ) # Match the column types of the datasets flu_data &lt;- coltrans(flu_data) dengue_data &lt;- coltrans(dengue_data) gapminder &lt;- gapminder %&gt;% transform(year = as.character(year)) # Filter gapminder to contain only the years present in the flu and dengue dataseets gapminder &lt;- gapminder %&gt;% filter(year %in% c(2002:2015) ) Using DBeaver SQL Following the cleaning of the data, the datasets need to be saved in R and uploaded to DBeaver for further review using these commands: #Save the data in .csv and .rds files write.csv(flu_data, &quot;data/data_raw/flu_data_tidy.csv&quot;) write.csv(dengue_data, &quot;data/data_raw/dengue_data_tidy.csv&quot;) write.csv(gapminder, &quot;data/data_raw/gapminder.csv&quot;) write_rds(flu_data, &quot;data/data_raw/flu_data_tidy.rds&quot;) write_rds(dengue_data, &quot;data/data_raw/dengue_data_tidy.rds&quot;) write_rds(gapminder, &quot;data/data_raw/gapminder.rds&quot;) #Upload the data to DBeaver dbWriteTable(con, &quot;flu_data&quot;, flu_data, overwrite = TRUE ) dbWriteTable(con, &quot;dengue_data&quot;, dengue_data, overwrite = TRUE ) dbWriteTable(con, &quot;gapminder&quot;, gapminder, overwrite = TRUE ) Uploading your files into DBeaver can come in handy to do things like: Using the SQL shell to obtain some information about the data in the tables And using the console to get information from the data Joining the data Getting information from the tables and forming them into something useful for visualizing the data can also be done using R. The Data needs to be joined together to really make some good informative figures. The next section shows how that was done plus a preview of the newly formed table. # Make the data tables joinable flu_data &lt;- datatrans(flu_data) dengue_data &lt;- datatrans(dengue_data) # Join the data tables flu_dengue_data &lt;- full_join(flu_data, dengue_data, by = c(&quot;country&quot;, &quot;year&quot;), suffix = c(&quot;_flu&quot;, &quot;_dengue&quot;) ) gap_flu_dengue_data &lt;- inner_join(flu_dengue_data, gapminder, by = c(&quot;country&quot;, &quot;year&quot;) ) knitr::kable(gap_flu_dengue_data %&gt;% head(5)) country year searches_flu searches_dengue infant_mortality life_expectancy fertility population gdp continent region Argentina 2002 NA NA 17.1 74.3 2.38 37889443 242076212334 Americas South America Argentina 2003 NA NA 16.6 74.5 2.34 38309475 263468585945 Americas South America Argentina 2004 8843 1.53 16.0 75.0 2.31 38728778 287258675094 Americas South America Argentina 2005 8627 1.03 15.3 75.3 2.29 39145491 313626005874 Americas South America Argentina 2006 8966 1.01 14.6 75.3 2.27 39558750 340177780212 Americas South America Visualising the data Now the data is joined together in one table we will use it to create some figures: We will start by plotting the occurrences of flu and dengue per region to see if they are more common in one region compared to the other. # plot occurrences of flu and dengue per region flu_region &lt;- gap_flu_dengue_data %&gt;% ggplot() + geom_col(aes(x = region, y = searches_flu, fill = region) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Flu searches per region&quot;, subtitle = &quot;Searches of the word &#39;Flu&#39; per region&quot;, x = &quot;&quot;, y = &quot;Searches&quot; ) dengue_region &lt;- gap_flu_dengue_data %&gt;% ggplot() + geom_col(aes(x = region, y = searches_dengue, fill = region) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Dengue searches per region&quot;, subtitle = &quot;Searches of the word &#39;Dengue&#39; per region&quot;, x = &quot;&quot;, y = &quot;Searches&quot; ) grid.arrange(flu_region, dengue_region, nrow = 1) The data shows that dengue is most common in South America and South-Eastern Asia. It also has some pressence in Central America and Southern Asia but not so much in other regions. Flu appears to be most common in Eastern Europe, Western Europe and Northern America. Regions where its not very common are Northern Europe, South America and Southeren Europe. The second graph will portray the occurrences of flu and dengue in the Netherlands over the period of 2002 to 2015. # plot occurrences of flu and dengue in the Netherlands by year flu_region &lt;- gap_flu_dengue_data %&gt;% filter(country == &quot;Netherlands&quot;) %&gt;% ggplot() + geom_col(aes(x = year, y = searches_flu, fill = year) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Flu searches in the Netherlands&quot;, subtitle = &quot;Searches of the word &#39;Flu&#39; in the Netherlands&quot;, x = &quot;Year&quot;, y = &quot;Searches&quot; ) dengue_region &lt;- gap_flu_dengue_data %&gt;% filter(country == &quot;Netherlands&quot;) %&gt;% ggplot() + geom_col(aes(x = year, y = searches_dengue, fill = year) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Dengue searches in the Netherlands&quot;, subtitle = &quot;Searches of the word &#39;Dengue&#39; in the Netherlands&quot;, x = &quot;Year&quot;, y = &quot;Searches&quot; ) grid.arrange(flu_region, dengue_region, nrow = 1) The flu had the most occurrences in 2009 and 2013 and the least in 2004 and 2010. There is no flu data for 2002 and 2003. This could be because of a lack of data in these years. The data shows that Dengue has had no occurrences in the Netherlands between 2002 and 2015. The third and last graph will show the worldwide occurrences of flu and dengue on a year to year basis. Showing us what years these diseases were most common. # plot occurrences of flu and dengue per year flu_region &lt;- gap_flu_dengue_data %&gt;% ggplot() + geom_col(aes(x = year, y = searches_flu, fill = year) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Flu searches per year&quot;, subtitle = &quot;Worldwide searches of the word &#39;Flu&#39;&quot;, x = &quot;Year&quot;, y = &quot;Searches&quot; ) dengue_region &lt;- gap_flu_dengue_data %&gt;% ggplot() + geom_col(aes(x = year, y = searches_dengue, fill = year) ) + theme(axis.text.x = element_text(angle = 45, hjust = 1), legend.position = &quot;none&quot; ) + labs(title = &quot;Dengue searches per year&quot;, subtitle = &quot;Worldwide searches of the word &#39;Dengue&#39;&quot;, x = &quot;Year&quot;, y = &quot;Searches&quot; ) grid.arrange(flu_region, dengue_region, nrow = 1) The Flu has the most occurrences in 2009 same as in the Netherlands as shown in the previous graph. The least occurrences were in 2002, 2003 and 2004. This could be because of a lack of data in these years. The rest of the datapoints show only minor peaks and valleys. Dengue has the most occurrences in 2009 and 2010 and the least in 2002, 2003 and 2015. This could also be because of a lack of data in the first and last few years of the survey. "],["parameterized-data.html", "Parameterized data", " Parameterized data In this excercise I will use the COVID-19 case data from the European Center for Disease Control (ECDC). Information about the data can be found here This data will be used to create a parameterized report of COVID-19 cases and deaths in the Netherlands from the first data in March 2021 to now. May 1st was excluded from this range because this date had an abnormally high value. This is probably because of it being a sum of all the data preceding May 2021. # Filter the data. The first and last line change the date format so the first column can be specified as such lct &lt;- Sys.getlocale(&quot;LC_TIME&quot;); Sys.setlocale(&quot;LC_TIME&quot;, &quot;C&quot;) data(list = params$data) covid_data &lt;- read_csv(params$data) covid_data_filtered &lt;- covid_data %&gt;% filter(countriesAndTerritories %in% &quot;Netherlands&quot;) %&gt;% transform(dateRep = as_date(dateRep, format = &quot;%d/%m/%Y&quot;) ) covid_data_filtered &lt;- covid_data_filtered[-c(110),] Sys.setlocale(&quot;LC_TIME&quot;, lct) # Make ggplot containing covid cases and deaths in the Netherlands during period Jan 2021 to May 2021 cases &lt;- covid_data_filtered %&gt;% ggplot(aes(x = dateRep, y = cases)) + geom_line(colour = &quot;blue&quot;, size = 2) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;COVID-19 cases in the Netherlands&quot;, subtitle = &quot;Recorded COVID-19 cases in the Netherlands by date&quot;, x = &quot;Month&quot;, y = &quot;Cases&quot; ) deaths &lt;- covid_data_filtered %&gt;% ggplot(aes(x = dateRep, y = deaths)) + geom_line(colour = &quot;red&quot;, size = 2) + theme(legend.position = &quot;none&quot;) + labs(title = &quot;COVID-19 deaths in the Netherlands&quot;, subtitle = &quot;Recorded COVID-19 deaths in the Netherlands by date&quot;, x = &quot;Month&quot;, y = &quot;Deaths&quot; ) cases deaths What stands out in the graphs is that both the COVID-19 cases and deaths start dropping towards July. This can be related to the large amount of people getting vaccinated in the Netherlands. Another thing that stands out is that even though the amount of cases go up until May, the amount of deaths does not. "],["looking-ahead.html", "Looking ahead", " Looking ahead In 2 years time I want to be working as a data scientist for a research group most preferably in the field of genomics or proteomics. My first big step and also my first big inspiration in regards to bioinformatics and data science is creating this digital portfolio as a result of the minor Data Science For Biology at the University of Applied Science Utrecht. The next skill for me to learn would be expanding my coding knowledge wherever necessary. I will start this by getting online certificates through the Coursera platform. Here, institutes like universities but also Google and IBM offer courses, certificate programs and degrees in a number of fields including data science. The first course I will participate in is the Genomic Data Science specialization including 8 courses covering genomic data science tools, working with Python for genomics and algorithms. After signing up you will be lead to a screen containing videos, readings and quizzes devided into weeks with an average workload of 1-2 hours. "]]
